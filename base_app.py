#streamlit dependencies

import streamlit as st
import joblib, os

## data dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import seaborn as sns
import re
# pip install nlppreprocess
from nlppreprocess import NLP
nlp = NLP()

def data_clean(line):
    #Removes urls, RT and white spaces
    line = re.sub(r'^RT ','', re.sub(r'https://t.co/\w+', '', line).strip()) 

    emojis = re.compile("["
                           u"\U0001F600-\U0001F64F"  # removes emojis,
                           u"\U0001F300-\U0001F5FF"  # removes pictures and symbols
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

    line = emojis.sub(r'', line)

    
    # Removes puctuation
    punctuation = re.compile("[.;:!\'’‘“”?,\"()\[\]]")
    tweet = punctuation.sub("", line.lower()) 

    # Removes stopwords
    nlp_for_stopwords = NLP(replace_words=True, remove_stopwords=True, 
                            remove_numbers=True, remove_punctuations=False) 
    tweet = nlp_for_stopwords.process(tweet) # This will remove stops words that are not necessary. 

    # tokenisation
    # We used the split method instead of the word_tokenise library because our tweet is already clean at this point
    # and the twitter data is not complicated
    tweet = tweet.split() 

    # POS 
    # Part of Speech tagging is essential to ensure Lemmatization perfoms well.
    pos = pos_tag(tweet)

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tweet = ' '.join([lemmatizer.lemmatize(word, po[0].lower()) 
                      if (po[0].lower() in ['n', 'r', 'v', 'a'] and word[0] != '@') else word for word, po in pos])

    return tweet


##reading in the raw data and its cleaner

vectorizer = open('resources/tfidfvect.pkl','rb')   ##  will be replaced by the cleaning and preprocessing function
tweet_cv = joblib.load(vectorizer)
# Data from train.csv
data = pd.read_csv('resources/train.csv')

def main():

    st.title("Welcome to our Tweet Classifier Project")
    st.subheader('Climate change tweet classification')
    st.image("Image.jpeg")
    

    ##creating a sidebar for selection purposes

    options = ["Information", "visualizations", "Prediction", "Team Members"]

    selection = st.sidebar.radio('Go to....', options)

    #st.sidebar.image(image, caption='Which Tweet are you?', use_column_width=True)



    ##information page

    if selection == 'Information':
        st.info('Information Section')
        st.write('Information section testing')
        st.markdown(""" we have created a machine learning classification model, that would be able to accurately classify if a person does, 
        or does not believe in climate change. We will be using message data generated by Twitter users for this purpose. 
        We will also be creating an accompanying application using Streamlit together with this model, to better represent our findings to environmentally conscious companies. 
        We will provide an accurate, efficient and easy to use solution to this challenge, which will also provide access to a vast number of diverse consumer sentiments. 
        This will greatly enhance the level of insight a company will have when creating marketing strategies in the future.
        """)

        raw = st.checkbox('Show the data')
        if raw:
            st.dataframe(data.head(10))

    ## Charts page

    if selection == 'visualizations':
        st.info("Visuals of charts from the data")

       # Number of Messages of each Sentiment
        st.write('Distribution of the sentiments')
        # Labeling the target
        data['sentiment'] = [['Negative', 'Neutral', 'Positive', 'News'][x+1] for x in data['sentiment']]
        
        # checking the distribution
        st.write('The numerical proportion of the sentiments')
        values = data['sentiment'].value_counts()/data.shape[0]
        labels = (data['sentiment'].value_counts()/data.shape[0]).index
        colors = ['lightsteelblue']
        plt.pie(x=values, labels=labels, autopct='%1.1f%%', startangle=90, explode= (0.04, 0, 0, 0))
        st.pyplot()
        st.set_option('deprecation.showPyplotGlobalUse', False)
        
        # checking the distribution
        sns.countplot(x='sentiment' ,data = data, palette=['#728CD4'])
        plt.ylabel('Count')
        plt.xlabel('Sentiment')
        plt.title('essages Count Per Sentiment')
        st.pyplot()
        st.set_option('deprecation.showPyplotGlobalUse', False)

        # top 10 Tags
        st.write('top ten tags from messages')
        data['users'] = [''.join(re.findall(r'@\w{,}', line)) if '@' in line else np.nan for line in data.message]
        sns.countplot(y="users", hue="sentiment", data=data,
                    order=data.users.value_counts().iloc[:10].index) 
        plt.ylabel('User')
        plt.xlabel('Number of Tags')
        plt.title('Top 10 Most Popular Tags')
        st.pyplot()

        st.markdown("Now we will look at the popular tages by sentiment:")

        # Generating graphs for the tags
        st.write('Positive sentiment tags')
        # Analysis of most popular tags, sorted by populariy
        sns.countplot(x="users", data=data[data['sentiment'] == 'Positive'],
                    order=data[data['sentiment'] == 'Positive'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 Positive Tags')
        plt.xticks(rotation=85)
        st.pyplot()

        # Analysis of most popular tags, sorted by populariy
        st.write("Negative sentiment tags")
        sns.countplot(x="users", data=data[data['sentiment'] == 'Negative'],
                    order=data[data['sentiment'] == 'Negative'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 Negative Tags')
        plt.xticks(rotation=85)
        st.pyplot()


        st.write("News sentiment tags")
        # Analysis of most popular tags, sorted by populariy
        sns.countplot(x="users", data=data[data['sentiment'] == 'News'],
                    order=data[data['sentiment'] == 'News'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 News Tags')
        plt.xticks(rotation=85)
        st.pyplot()

    ## prediction page

    if selection == 'Prediction':

        st.info('Predicting tweet messages based on the model used')

        data_source = ['Select option', 'Single text', 'Dataset'] ## differentiating between a single text and a dataset inpit

        source_selection = st.selectbox('What to classify?', data_source)

        # Load Our Models
        def load_prediction_models(model_file):
            loaded_models = joblib.load(open(os.path.join(model_file),"rb"))
            return loaded_models

        # Getting the predictions
        def get_keys(val,my_dict):
            for key,value in my_dict.items():
                if val == value:
                    return key


        if source_selection == 'Single text':
            ### SINGLE TWEET CLASSIFICATION ###
            st.subheader('tweet classification')

            input_text = st.text_area('Enter Text (max. 140 characters):') ##user entering a single text to classify and predict
            all_ml_models = ["LR","NB","SVC"]
            model_choice = st.selectbox("Choose ML Model",all_ml_models)

            prediction_labels = {'Negative':-1,'Neutral':0,'Positive':1,'News':2}
            if st.button('Classify'):

                st.text("Original test ::\n{}".format(input_text))
                text1 = data_clean(input_text) ###passing the text through the 'data_clean' function
                vect_text = tweet_cv.transform([text1]).toarray()
                if model_choice == 'LR':
                    predictor = load_prediction_models("resources/Logistic_regression.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)
                elif model_choice == 'SVC':
                    predictor = load_prediction_models("resources/SVC_model.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)
                elif model_choice == 'NB':
                    predictor = load_prediction_models("resources/tfidfvect.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)

                final_result = get_keys(prediction,prediction_labels)
                st.success("Tweet Categorized as:: {}".format(final_result))

        if source_selection == 'Dataset':
            ### DATASET CLASSIFICATION ###
            st.subheader('Dataset tweet classification')

            all_ml_models = ["LR","NB","SVC"]
            model_choice = st.selectbox("Choose ML Model",all_ml_models)

            prediction_labels = {'Negative':-1,'Neutral':0,'Positive':1,'News':2}
            text_input = st.file_uploader("Choose a CSV file", type="csv")
            if text_input is not None:
                text_input = pd.read_csv(text_input)

            #X = text_input.drop(columns='tweetid', axis = 1, inplace = True)   

            uploaded_dataset = st.checkbox('See uploaded dataset')
            if uploaded_dataset:
                st.dataframe(text_input.head(25))

            col = st.text_area('Enter column to classify')
            
            if st.button('Classify'):

                st.text("Original test ::\n{}".format(text_input))
                X1 = text_input[col].apply(data_clean) ###passing the text through the 'data_clean' function
                vect_text = tweet_cv.transform([X1]).toarray()
                if model_choice == 'LR':
                    predictor = load_prediction_models("resources/Logistic_regression.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)
                elif model_choice == 'SVC':
                    predictor = load_prediction_models("resources/SVC_model.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)
                elif model_choice == 'NB':
                    predictor = load_prediction_models("resources/NB_model.pkl")
                    prediction = predictor.predict(vect_text)
                    # st.write(prediction)

                text_input['sentiment'] = prediction
                final_result = get_keys(prediction,prediction_labels)
                st.success("Tweets Categorized as:: {}".format(final_result))

                
                csv = text_input.to_csv(index=False)
                b64 = base64.b64encode(csv.encode()).decode()  # some strings <-> bytes conversions necessary here
                href = f'<a href="data:file/csv;base64,{b64}">Download csv file</a>'

                st.markdown(href, unsafe_allow_html=True)


    ##contact page
    if selection == 'Team Members':
        st.info('The legends who made it happen')
        st.write('Jason Farrell')
        st.write('Melusi')
        st.write('Keletso')
        st.write('Cwaita')

        # Footer 
        st.image('resources/imgs/EDSA_logo.png')

if __name__ == '__main__':
	main()


