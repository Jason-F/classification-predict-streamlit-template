#streamlit dependencies

import streamlit as st
import joblib, os

## data dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
import seaborn as sns
import re
# pip install nlppreprocess
from nlppreprocess import NLP
nlp = NLP()

def data_clean(line):
    #Removes urls, RT and white spaces
    line = re.sub(r'^RT ','', re.sub(r'https://t.co/\w+', '', line).strip()) 

    emojis = re.compile("["
                           u"\U0001F600-\U0001F64F"  # removes emojis,
                           u"\U0001F300-\U0001F5FF"  # removes pictures and symbols
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

    line = emojis.sub(r'', line)

    
    # Removes puctuation
    punctuation = re.compile("[.;:!\'’‘“”?,\"()\[\]]")
    tweet = punctuation.sub("", line.lower()) 

    # Removes stopwords
    nlp_for_stopwords = NLP(replace_words=True, remove_stopwords=True, 
                            remove_numbers=True, remove_punctuations=False) 
    tweet = nlp_for_stopwords.process(tweet) # This will remove stops words that are not necessary. 

    # tokenisation
    # We used the split method instead of the word_tokenise library because our tweet is already clean at this point
    # and the twitter data is not complicated
    tweet = tweet.split() 

    # POS 
    # Part of Speech tagging is essential to ensure Lemmatization perfoms well.
    pos = pos_tag(tweet)

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tweet = ' '.join([lemmatizer.lemmatize(word, po[0].lower()) 
                      if (po[0].lower() in ['n', 'r', 'v', 'a'] and word[0] != '@') else word for word, po in pos])

    return tweet


##reading in the raw data and its cleaner

vectorizer = open('resources/tfidfvect.pkl','rb')   ##  will be replaced by the cleaning and preprocessing function
tweet_cv = joblib.load(vectorizer)
# Data from train.csv
data = pd.read_csv('resources/train.csv')

def main():

    st.title("Welcome to our Tweet Classifier Project")
    st.subheader('Climate change tweet classification')
    st.image("Image.jpeg", use_column_width=True)
    

    ##creating a sidebar for selection purposes

    options = ["Information", "Visualizations", "Prediction", "Team Members"]

    selection = st.sidebar.radio('Navigation Bar', options)



    ##information page

    if selection == 'Information':
        st.title('**Why is this app so Amazing?**')
        st.info('**Machine Learning is the the future for Business Strategy!**')
        st.markdown(""" We have created a machine learning classification model, that would be able to accurately classify if a person belief sentiment 
        in climate change. We will be using message data generated by Twitter users for this purpose. 
        We will also be creating an accompanying application using Streamlit together with this model, to better represent our findings to environmentally conscious companies. 
        We will provide an accurate, efficient and easy to use solution to this challenge, which will also provide access to a vast number of diverse consumer sentiments. 
        This will greatly enhance the level of insight a company will have when creating marketing strategies in the future.
        """)
        st.title('**Models used**')
        st.info('**Logistic Regression:**')
        st.markdown("""Logistic regression is a widely used machine learning model. Logistical regression does not directly model the response (in our case, a tweet message), 
        but rather models the probability that this response belongs to a specific sentiment. Logistic regression uses the sigmoid curve, which imposes a value between 0 and 1 
        to help classify these observations""")

        st.info('**Support Vector Classifier**')
        st.markdown("""The support vector machine will find the largest edge hyperplane for classification. 
        This means that it tries to find a hyperplane that maximizes the margin between the hyperplane and the observation, that is, it finds a plane 
        as far away from the observation as possible (this is indicated by the graph on the right). The maximum distance from the observation is considered the best 
        possible decision limit. Observations at the boundary are called support vectors, which can help the model find the best hyperplane.""")

        st.info('**Naive Bayes Multinomial**')
        st.markdown("""The Naive Bayes classifier assumes that the existence of a particular feature in a class is independent of the existence of any other features. 
        Use the prior probability and the probability that the observation belongs to a certain class to calculate the posterior probability that the observation belongs 
        to each class. Finally, the class with the highest posterior probability is the class to which the observation is classified to.""")

        st.subheader("Select the checkbox below to preview the dataset")
        raw = st.checkbox('Show the data')
        if raw:
            st.dataframe(data.head(10))

    ## Charts page

    if selection == 'Visualizations':
        st.title("**Graphs created from the data**")

       
        # Labeling the target
        data['sentiment'] = [['Negative', 'Neutral', 'Positive', 'News'][x+1] for x in data['sentiment']]
        
        # checking the distribution
        st.info('**The numerical proportion of the sentiments**')
        values = data['sentiment'].value_counts()/data.shape[0]
        labels = (data['sentiment'].value_counts()/data.shape[0]).index
        colors = ['lightsteelblue']
        plt.pie(x=values, labels=labels, autopct='%1.1f%%', startangle=90, explode= (0.04, 0, 0, 0))
        st.pyplot()
        st.set_option('deprecation.showPyplotGlobalUse', False)
        
        # checking the distribution
        # Number of Messages of each Sentiment
        st.info('**Distribution of the sentiments**')
        sns.countplot(x='sentiment' ,data = data, order=data.sentiment.value_counts().index)
        plt.ylabel('Count')
        plt.xlabel('Sentiment')
        plt.title('Messages Count Per Sentiment')
        st.pyplot()
        st.set_option('deprecation.showPyplotGlobalUse', False)

        # top 10 Tags
        st.info('**Top ten tags from tweets by Users**')
        data['users'] = [''.join(re.findall(r'@\w{,}', line)) if '@' in line else np.nan for line in data.message]
        sns.countplot(y="users", hue="sentiment", data=data,
                    order=data.users.value_counts().iloc[:10].index) 
        plt.ylabel('User')
        plt.xlabel('Number of Tags')
        plt.title('Top 10 Most Popular Tags')
        st.pyplot()

        st.subheader("**Now we will look at the popular user tags by sentiment**")

        # Generating graphs for the tags
        st.info('**Top 10 Positive Sentiment Tags**')
        # Analysis of most popular tags, sorted by populariy
        sns.countplot(x="users", data=data[data['sentiment'] == 'Positive'],
                    order=data[data['sentiment'] == 'Positive'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 Positive Tags')
        plt.xticks(rotation=85)
        st.pyplot()

        # Analysis of most popular tags, sorted by populariy
        st.info("**Top 10 Negative Sentiment Tags**")
        sns.countplot(x="users", data=data[data['sentiment'] == 'Negative'],
                    order=data[data['sentiment'] == 'Negative'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 Negative Tags')
        plt.xticks(rotation=85)
        st.pyplot()

        st.info("**Top 10 Neutral Sentiment Tags**")
        # Analysis of most popular tags, sorted by populariy
        sns.countplot(x="users", data=data[data['sentiment'] == 'Neutral'],
                    order=data[data['sentiment'] == 'Neutral'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 News Tags')
        plt.xticks(rotation=85)
        st.pyplot()

        st.info("**Top 10 News Sentiment Tags**")
        # Analysis of most popular tags, sorted by populariy
        sns.countplot(x="users", data=data[data['sentiment'] == 'News'],
                    order=data[data['sentiment'] == 'News'].users.value_counts().iloc[:10].index, palette=['#728CD4']) 

        plt.xlabel('User')
        plt.ylabel('Number of Tags')
        plt.title('Top 10 News Tags')
        plt.xticks(rotation=85)
        st.pyplot()

    ## prediction page

    if selection == 'Prediction':

        st.info('Predicting tweet messages based on the model used')

        #data_source = ['Select option', 'Single text', 'Dataset'] ## differentiating between a single text and a dataset inpit

        #source_selection = st.selectbox('What to classify?', data_source)

        # Load Our Models
        def load_prediction_models(model_file):
            loaded_models = joblib.load(open(os.path.join(model_file),"rb"))
            return loaded_models

        # Getting the predictions
        def get_keys(val,my_dict):
            for key,value in my_dict.items():
                if val == value:
                    return key
        st.subheader('tweet classification')
        input_text = st.text_area('Enter Text (max. 140 characters):') ##user entering a single text to classify and predict
        all_ml_models = ["LR","NB","SVC"]
        model_choice = st.selectbox("Choose ML Model",all_ml_models)
        prediction_labels = {'Negative':-1,'Neutral':0,'Positive':1,'News':2}
        if st.button('Classify'):
            st.text("Original test ::\n{}".format(input_text))
            text1 = data_clean(input_text) ###passing the text through the 'data_clean' function
            vect_text = tweet_cv.transform([text1]).toarray()
            if model_choice == 'LR':
                predictor = load_prediction_models("resources/Logistic_regression.pkl")
                prediction = predictor.predict(vect_text)
                # st.write(prediction)
            elif model_choice == 'SVC':
                predictor = load_prediction_models("resources/SVC_model.pkl")
                prediction = predictor.predict(vect_text)
                # st.write(prediction)
            elif model_choice == 'NB':
                predictor = load_prediction_models("resources/tfidfvect.pkl")
                prediction = predictor.predict(vect_text)
                # st.write(prediction)
            final_result = get_keys(prediction,prediction_labels)
            st.success("Tweet Categorized as:: {}".format(final_result))


    ##contact page
    if selection == 'Team Members':
        st.info('The legends who made it happen')
        st.write('Jason Farrell')
        st.write('Keletso Pule')
        st.write('Dineo Mahlangu')
        st.write('Melusi Zwane')
        st.write('Chwayita Happiness')
        st.image('resources/imgs/EDSA_logo.png',use_column_width=True)

if __name__ == '__main__':
	main()


